---
title: Mapping between Embeddings
params:
  qsize: 500 # size of image quadrant we subset to
  max_rank: 2000 # rank at which we start threholding
---

_Goal_: Map mass-spec measurements to plausible spatial expression patterns, based
on MIBI-ToF measurements, and provide some sense of uncertainty in the estimate.

_Outline_:

* For MIBI data, derive cell-level embeddings, based both on the cells' antigen
  expression levels and local spatial ecosystem.
    - This is the part of the analysis that is the most open-ended -- there are
    many ways to build embeddings from either quantitative matrices or images /
    graphs, and we also have to decide how to combine them.
    - We'll start with something simple: U-Map on quantitative and derived
    spatial features, with a parameter trading off the relative weights of the
    two tables.
    - Would be very interesting to try image (autoencoder) or graph (graph nn)
    features.
* Summarize MIBI samples based on these cell-level features. The idea is to
  cluster the embeddings and summarize a sample based on the fraction of cells
  that belong to each cluster.
* Relate mass-spec and MIBI measurements by learning a mapping from common
  features (9 shared antigens and overlapping patient characteristics) to the
  location on the embedding.
  - Our predicted spatial expression for a cytof sample is the spatial
    expression pattern associated with that region in the predicted embedding
    space.
  - We can gauge our ability to actually perform spatial expression imputation
    by (1) our prediction ability, from test mibitof samples to their true
    embeddings, and (2) a proportion-of-variance like quantity on the
    original embeddings[^1].

_Themes_:

* Try assembling easily re-usable components, and understanding their behavior
  when combined. This seems more broadly useful than trying to come up with a
  single model that cannot be reused elsewhere.
* The notion of a "sampling unit" is hierarchical (at least, it's not clear
  cut). We need to think of sampling and variation at both the patient and
  cellular level.
* The dichotomy between ML and stats is artificial, especially in applications
  like this, which are heavy in both non-tabular data and scientific questions.
  Liberally use methods from both disciplines.

```{r}
library("dplyr")
library("forcats")
library("ggplot2")
library("raster")
library("reshape2")
library("stars")
library("stringr")
library("tibble")
library("tidyr")
library("umap")
source("preprocessing.R")
theme_set(theme_bw() + theme(panel.grid=element_blank()))
```

Loading data...

```{r}
data_dir <- file.path("..", "Data")
load(file.path(data_dir, "mibiSCE.rda"))
tiff_paths <- list.files(file.path(data_dir, "TNBC_shareCellData"), "*.tiff", full=T)[1:25]
tiff_paths <- setdiff(tiff_paths, "../Data/TNBC_shareCellData/p30_labeledcellData.tiff") # we don't seem to have colData for this patient

sample_names <- str_extract(tiff_paths, "[0-9]+")

summary(mibi.sce)
colData(mibi.sce)$cell_type <- cell_type(mibi.sce)
glimpse(colData(mibi.sce))

## subset for testing
mibi.sce <- mibi.sce[, colData(mibi.sce)$SampleID %in% sample_names]
```

Now, we'll start implementing the outline proposed above, using simplest
components that we can find. We'll improve the parts that seem especially weak
in the second pass.

Also for the sake of simplicity, we'll work on subsamples of cells. We won't
randomly sample though, since that would lose interesting spatial variation --
instead we'll extract small quadrants from the full images.

# Subsampling

We'll loop over all the tiffs and extract the `r params$qsize` $\times$
`r params$qsize` top left quadrant from each image.

```{r}
ims <- list()
for (i in seq_along(tiff_paths)) {
  print(paste0(i, "/", length(tiff_paths)))
  r <- raster(tiff_paths[[i]])
  ims[[i]] <- crop(r, extent(1, params$qsize, 1, params$qsize))
}

names(ims) <- sample_names
cur_cells <- sapply(ims, raster::unique) %>%
  melt() %>%
  dplyr::rename(cellLabelInImage = "value", SampleID = "L1") %>%
  unite(sample_by_cell, SampleID, cellLabelInImage, remove=F)

colData(mibi.sce)$sample_by_cell <- colData(mibi.sce) %>%
                  as.data.frame() %>%
                  dplyr::select(SampleID, cellLabelInImage) %>%
                  unite(sample_by_cell, SampleID, cellLabelInImage) %>%
                  .[["sample_by_cell"]]

mibi.sce <- mibi.sce[, colData(mibi.sce)$sample_by_cell %in% cur_cells$sample_by_cell]
```

# Embeddings

Now, let's extract some features on which to perform the joint embedding. We'll
transform and reweight the columns, to make the two sets of features more
comparable. First, for transformation, we'll convert antigen expression values
to ranks and then threshold.

```{r}
mibi.sce <- quantile_transform(mibi.sce, params$max_rank)
x <- t(assay(mibi.sce))
x_order <- hclust(dist(t(x)))$order
pairs(x[, x_order[1:6]], col = rgb(0, 0, 0, 0.5), cex=0.1)
```

Next, we'll extract some features from the spatial data. Note that some cells
seem to appear in the raster but not in the `colData`. This seems weird, and is
worth looking into, but for now I'm going to just innerJoin to ignore that.

```{r}
# polygonize each raster
col_df <- as.data.frame(colData(mibi.sce)) %>%
  mutate(
    cell_group = fct_lump(cell_type, prop = 0.05)
  )

polys <- list()
for (i in seq_along(ims)) {
  cur_cols <- col_df %>%
    filter(SampleID %in% sample_names[i])

  polys[[i]] <- polygonize(ims[[i]]) %>%
    mutate(SampleID = as.numeric(sample_names[i])) %>%
    unite(sample_by_cell, SampleID, cellLabelInImage, remove=FALSE) %>%
    inner_join(cur_cols, by = c("sample_by_cell", "SampleID", "cellLabelInImage"))
}

# a little plot
polys_df <- do.call(rbind, polys)
ggplot(polys_df %>% filter(!is.na(cellSize),  SampleID %in% 1:16)) +
  geom_sf(aes(fill = as.factor(tumorYN))) +
  facet_wrap(Survival_days_capped_2016.1.1~SampleID) +
  theme(legend.position = "bottom")

# some features that don't need graph construction
cell_stats <- polys_df %>%
  dplyr::select(sample_by_cell, cell_type, cellSize) %>%
  mutate(
    log_size = log(cellSize),
    value = 1
  ) %>%
  spread(cell_type, value, 0) %>%
  as.data.frame() %>%
  dplyr::select(-cellSize, -geometry)

# extract basic graph features
graph_stats <- list()
for (i in seq_along(polys)) {
  print(paste0("processing sample ", i, "/", length(polys)))
  cell_ids <- unique(polys[[i]]$cellLabelInImage)
  G <- extract_graph(cell_ids, polys[[i]]$geometry)
  graph_stats[[i]] <- loop_stats(cell_ids, "graph", G, polys[[i]], typeProps) %>%
    mutate(cellType = paste0("graph_neighbors_", cellType)) %>%
    spread(cellType, props, 0)
}

names(graph_stats) <- sample_names
graph_stats <- bind_rows(graph_stats, .id = "SampleID") %>%
  mutate_all(replace_na, 0) %>%
  unite(sample_by_cell, SampleID, cellLabelInImage) %>%
  left_join(as.data.frame(cell_stats))

# example plot: what are the neighborhoods of tumors?
plot(graph_stats$`Keratin-positive tumor` + runif(nrow(graph_stats), -0.25, 0.25),
     graph_stats$`graph_neighbors_Keratin-positive tumor`)
```

Now, we'll standardize these features and learn some embeddings.

```{r}
# standardize features between the two tables
x <- t(assay(mibi.sce)) %>%
  as_tibble()
x$sample_by_cell <- col_df$sample_by_cell
x <- x %>%
  mutate_at(vars(-sample_by_cell), function(u) (u - min(u)) / diff(range(u))) %>%
  mutate_at(vars(-sample_by_cell), function(u) u / sqrt(ncol(x)))

y <- graph_stats %>%
  mutate_at(vars(-sample_by_cell), function(u) (u - min(u)) / diff(range(u))) %>%
  mutate_at(vars(-sample_by_cell), function(u) u / sqrt(ncol(graph_stats) - 1)) %>%
  select_if(function(u) !all(is.na(u)))

z <- x %>% inner_join(y, by = "sample_by_cell")
z_mat <- z %>%
  dplyr::select(-sample_by_cell) %>%
  as.matrix()
heatmap(z_mat)

# learning embeddings across the two tables
embeddings <- umap(z_mat)
embeddings_df <- embeddings$layout %>%
  as_tibble(.name_repair = "universal") %>%
  rename(`...1` = "l1", `...2` = "l2") %>%
  mutate(sample_by_cell = z$sample_by_cell) %>%
  left_join(col_df) %>%
  left_join(z)

```

We'll plot the embeddings we just made, against some of the derived features.

```{r}
ggplot(embeddings_df %>% filter(SampleID < 23)) +
  geom_hline(yintercept = 0) +
  geom_vline(xintercept = 0) +
  geom_point(
    aes(x = l1, y = l2, col = cell_group),
    size = 0.5, alpha = 0.7
  ) +
  facet_wrap(~SampleID) +
  scale_color_brewer(palette = "Set2") +
  guides(col = guide_legend(override.aes = list(alpha = 1, size = 5))) +
  ggtitle("Cell Types") +
  theme(legend.position = "bottom")

ggplot(embeddings_df %>% filter(SampleID < 23)) +
  geom_point(aes(x = l1, y = l2, col = Fe)) +
  facet_wrap(~ SampleID)

ggplot(embeddings_df %>% filter(SampleID < 23)) +
  geom_hline(yintercept = 0) +
  geom_vline(xintercept = 0) +
  geom_point(
    aes(x = l1, y = l2, col = sqrt(graph_neighbors_Macrophages)),
    size = 0.5, alpha = 0.7
  ) +
  scale_color_viridis(option = "inferno") +
  facet_wrap(~ SampleID) +
  ggtitle("Prop. Neighbors are Macrophages") +
  theme(legend.position = "bottom")
```

# Cells $\to$ Samples

Now that we have embeddings at the cell level, we can try to summarize samples
by how many of their cells lie in different regions of the embedding space.
First, we'll cluster using an arbitrary $K$.

```{r}
clusters <- kmeans(embeddings$layout, centers = 10)

# plot the clusters
ggplot(embeddings_df) +
  geom_point(
    aes(x = l1, y = l2, col = cell_group),
    size = 0.5, alpha = 0.7
  ) +
  geom_text(
    data = data.frame(clusters$centers, cluster = seq_len(nrow(clusters$centers))),
    aes(x = X1, y = X2, label = cluster), size = 5,
  ) +
  geom_hline(yintercept = 0) +
  geom_vline(xintercept = 0) +
  scale_color_brewer(palette = "Set2") +
  guides(col = guide_legend(override.aes = list(alpha = 1, size = 5))) +
  ggtitle("Clusters vs. Cell Types") +
  theme(legend.position = "bottom")

# summarize the samples
embeddings_df$cluster <- as_factor(clusters$cluster)
cluster_props <- embeddings_df %>%
  group_by(SampleID, cluster) %>%
  summarise(count = n()) %>%
  group_by(SampleID) %>%
  mutate(
    total = sum(count),
    prop = count / total
  )

# order the clusterings
cluster_mat <- cluster_props %>%
  dplyr::select(-count, -total) %>%
  spread(cluster, prop, 0)
cluster_order <- hclust(
  dist(as.matrix(cluster_mat[, -1]))
)$order

cluster_props <- cluster_props %>%
  mutate(
    SampleID = factor(SampleID, cluster_mat[[1]][cluster_order])
  )

grade <- tibble(col_df) %>%
  dplyr::select(SampleID, GRADE) %>%
  mutate(
    SampleID = factor(SampleID, levels(cluster_props$SampleID))
  ) %>%
  unique()

ggplot(cluster_props %>% left_join(grade)) +
  geom_bar(
    aes(x = SampleID, y = prop, fill = cluster),
    stat = "identity"
  ) +
  facet_grid(.~GRADE, scales="free_x", space="free_x") +
  scale_fill_brewer(palette = "Set3")
```

# Guessing the Embedding

We'll finally load the cytof data to identify shared features. Then, we'll look
at the relationship between those features and the above embeddings, to see if
we could map the cytof data into the mibitof embedding space.

```{r}
load(file.path(data_dir, "masstagSCE.rda"))
masstag <- data_list("sce")
```


[^1]: We can try predicting different sample characteristics from the
    embeddings, for example. It seems like what people do by eye anyways (trying
    to tell whether known group separate).
